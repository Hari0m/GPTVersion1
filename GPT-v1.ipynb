{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5404aa1-f3d3-429e-9b1a-e824b6ef70b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import mmap\n",
    "import random\n",
    "import pickle\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "block_size = 32\n",
    "batch_size = 128\n",
    "max_iters = 200\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 100\n",
    "eval_interval = 500\n",
    "n_embd = 384\n",
    "n_layer = 1\n",
    "n_head = 1\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26abb177-673c-4a07-9e40-3d73f3fd6371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea85cb15-7a06-43f2-b904-a35456a88c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = \"\"\n",
    "with open('vocab.txt','r',encoding = 'utf-8') as f:\n",
    "    text = f.read()\n",
    "    chars = sorted(list(set(text)))\n",
    "\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "626caaee-cc36-4498-b11f-17f69455e15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_int = {ch:i for i,ch in enumerate(chars)}\n",
    "int_to_string = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "\n",
    "# data = torch.tensor(encode(text),dtype=torch.long)\n",
    "# print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8db1ff94-f54f-4112-a58a-4abeba761226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = int(0.8*len(data))\n",
    "# train_data = data[:n]\n",
    "# val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f27e5514-01d2-485d-857f-b2aaa4dfd1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#memory map for using small snipets of text from a single file of any size\n",
    "\n",
    "def get_random_chunk(split):\n",
    "    filename = \"output_train.txt\" if split == \"train\" else \"output_val.txt\"\n",
    "    with open(filename,'rb') as f:\n",
    "        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:\n",
    "            #Determine the file size and a random position to start reading\n",
    "            file_size = len(mm)\n",
    "            start_pos = random.randint(0, (file_size)-block_size*batch_size)\n",
    "            #Seek to the random position and read the chunk\n",
    "            mm.seek(start_pos)\n",
    "            block = mm.read(block_size*batch_size-1)\n",
    "            #Decode the chunk to a string, ignoring any invalid byte sequences\n",
    "            decoded_block = block.decode('utf-8', errors='ignore').replace('\\r', '')\n",
    "\n",
    "            #Train and test splits\n",
    "            data = torch.tensor(encode(decoded_block), dtype=torch.long)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82319a6d-2f14-4896-81d4-658876911fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    # data = train_data if split == 'train' else val_data\n",
    "    data = get_random_chunk(split)\n",
    "    ix = torch.randint(len(data)-block_size,(batch_size,))\n",
    "    # print(ix)\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x,y = x.to(device),y.to(device)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df3b3e32-7961-4cfa-82b8-f2586c0c6c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train','val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X,Y = get_batch(split)\n",
    "            logits,loss = model(X,Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab68897-06d2-4b32-a230-66ddefe7ca7d",
   "metadata": {},
   "source": [
    "## Note: sinusoidal functions are used in base transformer model while learned embeddings like the ones used here are used in variants like GPT. So better performance would be shown for our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43df30c7-9f8c-4557-8514-f59101ba534f",
   "metadata": {},
   "source": [
    "Note: Multihead attention allows us to hear different parts of the conversation between the heads and scaling(1/root of length of key) helps to head all parts evenly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab32593c-8696-4c64-ba15-3c5efcbf066a",
   "metadata": {},
   "source": [
    "Note: Module list does not one layer/ head after another, but rather each is isolated and gets it unique perspective. Simultaneous computation. Uses GPU capabilities.\n",
    "Sequential processing is where one block depends on another to synchronously complete. We are waiting on one to finish before we move to next one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61ea5b42-6df7-495c-b2ed-74c37c4ba3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model parameters...\n",
      "loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"one head of self-attention\"\"\"\n",
    "    def __init__(self,head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias= False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) #this just registers a no look ahead masking in the model state. Reduces computation.\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self,x):\n",
    "        # input of size (batch,time-step,channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)  # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1)*k.shape[-1]**-0.5 #(B,T,HS) @ (B,hs,T) -> (B,T,T)\n",
    "        wei = wei.masked_fill(self.tril[:T,:T] == 0, float('-inf')) #(B,T,T)\n",
    "        wei = F.softmax(wei,dim = -1) #(B,T,T)\n",
    "        wei = self.dropout(wei)\n",
    "        #perform the weighted aggregation of the values\n",
    "        v = self.value(x) #(B,T,hs)\n",
    "        out = wei @ v #(B,T,T) @ (B,T,hs) -> (B,T,hs)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self attention in parallel\"\"\"\n",
    "    def __init__(self,num_heads,head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size*num_heads,n_embd) #adds more learnable parameters to help the network learn more about the text\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self,x):\n",
    "        out = torch.cat([h(x) for h in self.heads],dim = -1) #(B, T, F)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "                \n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity\"\"\"\n",
    "    def __init__(self,n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(n_embd, 4*n_embd),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(4*n_embd,n_embd),\n",
    "                                 nn.Dropout(dropout)) #causes a percentage of neurons to dropout and become 0. Prevents overfitting\n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self,n_embd,n_head):\n",
    "        #n_embd: embedding dimension, n_head = the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd//n_head\n",
    "        self.sa = MultiHeadAttention(n_head,head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.sa(x)\n",
    "        x = self.ln1(x+y)\n",
    "        y = self.ffwd(x)\n",
    "        x = self.ln2(x+y)\n",
    "        return x\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size,n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size,n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd,n_head = n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) #final layer norm ( can be used to make model converge better)\n",
    "        self.lm_head = nn.Linear(n_embd,vocab_size)\n",
    "\n",
    "        self.apply(self.__init__weights)\n",
    "    def __init__weights(self,module):\n",
    "        if isinstance(module,nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean = 0.0, std = 0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module,nn.Embedding):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std = 0.02)\n",
    "    \n",
    "    def forward(self,index,targets = None):\n",
    "\n",
    "        B,T = index.shape\n",
    "        \n",
    "        #idx and targets are both (B,T) tensor of intergers \n",
    "        tok_emb = self.token_embedding_table(index)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T,device = device)) #(T,C)\n",
    "        x = tok_emb + pos_emb #(B,T,C)\n",
    "        x = self.blocks(x) #(B,T,C)\n",
    "        x = self.ln_f(x) #(B,T,C)\n",
    "        logits = self.lm_head(x) #(B,T,vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T,C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits,targets)\n",
    "            \n",
    "        return logits,loss\n",
    "    def generate(self,index,max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits,loss = self.forward(index)\n",
    "            # focus on only thelast time step\n",
    "            logits = logits[:,-1,:]\n",
    "            # apply softmax to get the probabilities\n",
    "            probs = F.softmax(logits,dim = -1)\n",
    "            # sample from the distribution\n",
    "            index_next = torch.multinomial(probs,num_samples = 1)\n",
    "            index = torch.cat((index,index_next),dim = 1)\n",
    "        return index\n",
    "model = GPTLanguageModel(vocab_size)\n",
    "print(\"loading model parameters...\")\n",
    "\n",
    "# context = torch.zeros((1,1),dtype = torch.long,device = device)\n",
    "# generated_chars = decode(m.generate(context, max_new_tokens = 500)[0].tolist())\n",
    "# print(generated_chars)\n",
    "\n",
    "with open('model-01.pkl','rb') as f:\n",
    "    model = pickle.load(f)\n",
    "print(\"loaded successfully!\")\n",
    "m = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd39f627-5767-4809-b926-c5694fdf350a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train loss 2.2247, val loss: 2.3066\n",
      "step: 100, train loss 2.1129, val loss: 2.0858\n",
      "2.2786340713500977\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(),lr=learning_rate)\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step: {iter}, train loss {losses['train']:.4f}, val loss: {losses['val']:.4f}\")\n",
    "\n",
    "    #sample a batch of data\n",
    "    xb,yb = get_batch('train')\n",
    "\n",
    "    #evaluate the loss\n",
    "    logits,loss= model.forward(xb,yb)\n",
    "    optimizer.zero_grad(set_to_none = True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())\n",
    "\n",
    "with open('model-01.pkl','wb') as f:\n",
    "    pickle.dump(model,f)\n",
    "print('model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1a7c64-046d-48e5-8bc3-496bce57c56f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
